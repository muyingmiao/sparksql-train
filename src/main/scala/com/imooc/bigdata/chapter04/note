SparkSession
    Spark Core: SparkContext
    Spark SQL: 难道就没有SparkContext？
    2.x之后统一的

    1.x里面Spark SQL的编程的入口点：SQLContext HiveContext



DataFrame是什么？
    R/Pandas
    SchemaRDD

    RDD/MR  门槛
    R/Pandas 数据分析  ✅   单机

    A Dataset is a distributed collection of data
    A DataFrame is a Dataset organized into named columns
        以列（列名、列类型、列值）的形式构成的分布式数据集
              id   int    10
              name string pk
    It is conceptually equivalent to a table in a relational database or a data frame in R/Python
        DF = Table  ==> SQL

    In Scala and Java, a DataFrame is represented by a Dataset of Rows

    提供了很多方法的支持

DataFrame API
    遇到问题，或者是自己有想法想不通或者想不明白，怎么办？
    ==> 动手测试



Dataset




DataFrame vs Dataset
    SchemaRDD ==> DataFrame  ==>（compile-time type safety）==> Dataset
    DataFrame = Dataset[Row]  untype类型
    Dataset是一种强类型  typed类型







在日常开发过程中，我们使用Spark SQL来进行日志处理（90%）
你要处理一个目录下或者指定文件的日志数据，数据格式是文本类型的
直接使用spark.read.text(path)读进来之后，就是只有一个string类型的名字为value的值

1）uses reflection to infer the schema of an RDD that contains specific types of objects
2）creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD

对于字段比较少的场景，个人倾向于使用第一种
对于字段比较多的场景，个人倾向于使用第二种，自己灵活定制





编程方式实现的三步曲
1）Create an RDD of Rows from the original RDD;
2）Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
3）Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.





